# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

# Defines the trigger for the pipeline.
# This pipeline will only run when changes are made on the master branch and in the 'global' path.
trigger:
  branches:
    include:
    - master
  paths:
    include:
    - global/*

# Specifies the pool of virtual machines to use for the pipeline.
# In this case, we're using the latest Ubuntu VM image.
pool:
  vmImage: ubuntu-latest

steps:
- checkout: self
  fetchDepth: 2 # Fetches the last 2 commits.

- script: |
    # Get the list of files changed in the last commit.
    FILES=$(git diff --name-only HEAD~1 HEAD)
    # Store the list in a pipeline variable 'changedFiles'.
    echo "##vso[task.setvariable variable=changedFiles]$FILES"
  displayName: 'Get changed files'

- script: |
    # Print the names of changed files.
    echo $(changedFiles)
  displayName: 'Display changed files'

- script: |
    cat $(changedFiles)
  displayName: 'View content of changedFiles'

- script: |
    # Check if the changed file(s) is in the 'global' directory.
    # If so, extract the filename and directory parts, and store them as pipeline variables.
    if [[ "$(changedFiles)" == global* ]]; then
      echo "##vso[task.setvariable variable=startsWithGlobal]true"
      LAST_PART=$(basename $(changedFiles))
      echo "##vso[task.setvariable variable=lastPart]$LAST_PART"
      FIRST_PART=${LAST_PART:0:-33}
      echo "##vso[task.setvariable variable=firstPart]$FIRST_PART"
    else
      # If not, set the 'startsWithGlobal' pipeline variable to 'false' and exit the script.
      echo "##vso[task.setvariable variable=startsWithGlobal]false"
      exit 1
    fi
  displayName: 'Check if changedFiles starts with "global"'

- script: |
    # Install the lxml Python library, used for parsing XML files.
    pip install lxml
  displayName: 'Install lxml'

- script: |
    # Use Python to parse the XML file and extract the content inside <script> tags.
    # The content is then hashed, stored in a pipeline variable, and written to a new .js file.
    # The original <script> tag in the XML file is replaced with the hash value.
    python -c "
    import lxml.etree as ET
    import hashlib

    def remove_cdata_tags(text):
        return text.replace('<![CDATA[', '').replace(']]>', '')

    def hash_content(content):
        return hashlib.sha256(content.encode()).hexdigest()

    parser = ET.XMLParser(remove_blank_text=True)
    tree = ET.parse('$(changedFiles)', parser)
    root = tree.getroot()

    for script in root.iter('script'):
        cdata = script.text  # get CDATA content
        if cdata:
            # print(cdata)
            cdata_content = remove_cdata_tags(cdata)  # remove CDATA tags
            print(cdata_content)
            hash_value = hash_content(cdata_content)  # get SHA-256 hash of the content
            script.text = hash_value  # replace the content of the script tag with the hash value
            print(hash_value)
            # with open('script.js', 'w') as f:
            with open(f'{hash_value}.js', 'w') as f:
                f.write(cdata_content + '\n')

        # Set an Azure Pipelines variable with the hash value
        print(f'##vso[task.setvariable variable=hashValue]{hash_value}')

    # Write the modified XML to a new file
    tree.write('$(lastPart)', pretty_print=True, xml_declaration=True, encoding='utf-8')
    "
  displayName: 'Extract <script> content to script.js'


- script: |
    echo $(hashValue)
  displayName: 'Print hashValue'


- script: |
    echo "Current Directory:"
    pwd
    echo "List files:"
    ls -la
    cat $(hashValue).js
    cat $(lastPart)
  displayName: 'View content of script and modified.xml'


- task: ServiceNow-DevOps-Config-Agent-Upload-Config@1
#  condition: eq(variables['startsWithGlobal'], 'true')
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    uploadTarget: 'component'
    configFile: '$(lastPart)'
    namePath: '$(firstPart)'
    dataFormat: 'xml'
    autoValidate: true
    autoCommit: true
    convertPath: true


- task: ServiceNow-DevOps-Config-Agent-Export-Snapshot@1
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    deployableName: 'TestJS'
    exporterName: 'returnESLint'
    dataFormat: 'json'
    saveFile: true
    fileName: 'config_eslint.json'


- task: CmdLine@2
  inputs:
    script: |
      echo Tree
      tree $(Pipeline.Workspace)
  displayName: 'List content'


- script: |
    hexdump -C $(Pipeline.Workspace)/config_eslint.json
  displayName: 'View content of config_eslint.json'


- script: |
    npm install eslint
  displayName: 'Install ESLint'

- script: |
    npx eslint -c $(Pipeline.Workspace)/config_eslint.json $(hashValue).js || true
  displayName: 'ESLint Check'

- script: |
    npm install prettier
  displayName: 'Install Prettier'

# - script: |
#     npx prettier --check $(hashValue).js
#   displayName: 'Prettier Code Formatting'

- script: |
    npx prettier $(hashValue).js
  displayName: 'Prettier Code Formatting'

- script: |
    npx prettier --write $(hashValue).js
  displayName: 'Prettier Code Formatting'

- script: |
    cat $(hashValue).js
  displayName: 'Print final'


- task: ServiceNow-DevOps-Config-Agent-Export-Snapshot@1
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    deployableName: 'TestJS'
    exporterName: 'returnSonarQube'
    dataFormat: 'json'
    saveFile: true
    fileName: 'sonar_qube.json'

- script: |
    cat -v $(Pipeline.Workspace)/sonar_qube.json
  displayName: 'View content of sonar_qube.json'

- script: |
    hexdump -C $(Pipeline.Workspace)/sonar_qube.json
  displayName: 'View content of sonar_qube.json using hexdump'

- task: SonarQubePrepare@4
  inputs:
    SonarQube: 'sonarqubedemo'
    scannerMode: 'CLI'
    configMode: 'manual'
    cliProjectKey: 'algimas'
    cliProjectName: 'algimas'
    cliSources: '$(Pipeline.Workspace)/s'
    extraProperties: |
      sonar.host.url=http://algis-sonarqube.polandcentral.azurecontainer.io:9000
      sonar.login=varrrr

- task: SonarQubeAnalyze@4

- task: SonarQubePublish@4
  inputs:
    pollingTimeoutSec: '300'

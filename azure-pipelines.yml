# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

# Defines the trigger for the pipeline.
# This pipeline will only run when changes are made on the master branch and in the 'global' path.
trigger:
  branches:
    include:
    - master
  paths:
    include:
    - global/*

# Specifies the pool of virtual machines to use for the pipeline.
# In this case, we're using the latest Ubuntu VM image.
pool:
  vmImage: ubuntu-latest

steps:
- checkout: self
  fetchDepth: 2 # Fetches the last 2 commits.

- script: |
    # Get the list of files changed in the last commit.
    FILES=$(git diff --name-only HEAD~1 HEAD)
    # Store the list in a pipeline variable 'changedFiles'.
    echo "##vso[task.setvariable variable=changedFiles]$FILES"
  displayName: 'Get changed files'

- script: |
    # Print the names of changed files.
    echo $(changedFiles)
  displayName: 'Display changed files'

- script: |
    cat $(changedFiles)
  displayName: 'View content of changedFiles'

- script: |
    # Check if the changed file(s) is in the 'global' directory.
    # If so, extract the filename and directory parts, and store them as pipeline variables.
    if [[ "$(changedFiles)" == global* ]]; then
      echo "##vso[task.setvariable variable=startsWithGlobal]true"
      LAST_PART=$(basename $(changedFiles))
      echo "##vso[task.setvariable variable=lastPart]$LAST_PART"
      FIRST_PART=${LAST_PART:0:-33}
      echo "##vso[task.setvariable variable=firstPart]$FIRST_PART"
    else
      # If not, set the 'startsWithGlobal' pipeline variable to 'false' and exit the script.
      echo "##vso[task.setvariable variable=startsWithGlobal]false"
      exit 1
    fi
  displayName: 'Check if changedFiles starts with "global"'

- script: |
    # Install the lxml Python library, used for parsing XML files.
    pip install lxml
  displayName: 'Install lxml'

- script: |
    # Use Python to parse the XML file and extract the content inside <script> tags.
    # The content is then hashed, stored in a pipeline variable, and written to a new .js file.
    # The original <script> tag in the XML file is replaced with the hash value.
    python -c "
    import lxml.etree as ET
    import hashlib

    def remove_cdata_tags(text):
        return text.replace('<![CDATA[', '').replace(']]>', '')

    def hash_content(content):
        return hashlib.sha256(content.encode()).hexdigest()

    parser = ET.XMLParser(remove_blank_text=True)
    tree = ET.parse('$(changedFiles)', parser)
    root = tree.getroot()

    for script in root.iter('script'):
        cdata = script.text  # get CDATA content
        if cdata:
            # print(cdata)
            cdata_content = remove_cdata_tags(cdata)  # remove CDATA tags
            print(cdata_content)
            hash_value = hash_content(cdata_content)  # get SHA-256 hash of the content
            script.text = hash_value  # replace the content of the script tag with the hash value
            print(hash_value)
            # with open('script.js', 'w') as f:
            with open(f'{hash_value}.js', 'w') as f:
                f.write(cdata_content + '\n')

        # Set an Azure Pipelines variable with the hash value
        print(f'##vso[task.setvariable variable=hashValue]{hash_value}')

    # Write the modified XML to a new file
    tree.write('$(lastPart)', pretty_print=True, xml_declaration=True, encoding='utf-8')
    "
  displayName: 'Extract <script> content to script.js'


- script: |
    echo $(hashValue)
  displayName: 'Print hashValue'


- script: |
    echo "Current Directory:"
    pwd
    echo "List files:"
    ls -la
    cat $(hashValue).js
    cat $(lastPart)
  displayName: 'View content of script and modified.xml'


- task: ServiceNow-DevOps-Config-Agent-Upload-Config@1
#  condition: eq(variables['startsWithGlobal'], 'true')
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    uploadTarget: 'component'
    configFile: '$(lastPart)'
    namePath: '$(firstPart)'
    dataFormat: 'xml'
    autoValidate: true
    autoCommit: true
    convertPath: true


- task: ServiceNow-DevOps-Config-Agent-Export-Snapshot@1
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    deployableName: 'TestJS'
    exporterName: 'returnESLint'
    dataFormat: 'json'
    saveFile: true
    fileName: 'config_eslint.json'


- task: CmdLine@2
  inputs:
    script: |
      echo Tree
      tree $(Pipeline.Workspace)
  displayName: 'List content'

- task: CmdLine@2
  inputs:
    script: |
      echo $(System.DefaultWorkingDirectory)
  displayName: 'System.DefaultWorkingDirectory'


- script: |
    hexdump -C $(Pipeline.Workspace)/config_eslint.json
  displayName: 'View content of config_eslint.json'

- task: CopyFiles@2
  inputs:
    SourceFolder: '$(System.DefaultWorkingDirectory)/'
    Contents: 'sn_eslint_custom_rule.js'
    TargetFolder: '/home/vsts/work/node_modules/eslint-plugin-sn_eslint_custom_rule/lib/rules/'
    OverWrite: true

- script: |
    cat /home/vsts/work/node_modules/eslint-plugin-sn_eslint_custom_rule/lib/rules/sn_eslint_custom_rule.js
  displayName: 'View content of sn_eslint_custom_rule.js'


- script: |
    npm install eslint
  displayName: 'ESLint Install'

#npx eslint -c $(Pipeline.Workspace)/config_eslint.json $(hashValue).js || true

# - script: |
#     (cd /home/vsts/work/node_modules/eslint-plugin-sn_eslint_custom_rule/lib/rules && npm link)
#     npm link eslint-plugin-sn_eslint_custom_rule
#   displayName: 'ESLint Linking'

- script: |
    cd /home/vsts/work/node_modules/eslint-plugin-sn_eslint_custom_rule/lib/rules
    npm link
    cd $(System.DefaultWorkingDirectory)
    npm link eslint-plugin-sn_eslint_custom_rule
    cd $(Pipeline.Workspace)
    npm link eslint-plugin-sn_eslint_custom_rule
  displayName: 'ESLint Linking'

- script: |
    npx eslint -c $(Pipeline.Workspace)/s/sn_eslint_config.json $(hashValue).js || true
  displayName: 'ESLint Check'

# - task: CmdLine@2
#   inputs:
#     script: |
#       echo Tree
#       tree $(Pipeline.Workspace)
#   displayName: 'List content one more time'


# - script: |
#     echo 'const path = require("path");
#     const modulePath = require.resolve("eslint");
#     const nodeModulesDir = path.dirname(modulePath);
#     console.log(nodeModulesDir);' > get_node_modules_dir.js
#     node get_node_modules_dir.js
#   displayName: 'Print Node Modules Directory'

# - task: CmdLine@2
#   inputs:
#     script: |
#       echo Tree
#       tree /home/vsts/work/node_modules
#   displayName: 'List node_modules'



- script: |
    npm install prettier
  displayName: 'Install Prettier'

# - script: |
#     npx prettier --check $(hashValue).js
#   displayName: 'Prettier Code Formatting'

- script: |
    npx prettier $(hashValue).js
  displayName: 'Prettier Code Formatting'

- script: |
    npx prettier --write $(hashValue).js
  displayName: 'Prettier Code Formatting'

- script: |
    cat $(hashValue).js
  displayName: 'Print final'


- task: ServiceNow-DevOps-Config-Agent-Export-Snapshot@1
  inputs:
    connectedServiceName: 'snow8'
    applicationName: 'Snow8Demo'
    deployableName: 'TestJS'
    exporterName: 'returnSonarQube'
    dataFormat: 'json'
    saveFile: true
    fileName: 'sonar_qube.json'

- script: |
    cat -v $(Pipeline.Workspace)/sonar_qube.json
  displayName: 'View content of sonar_qube.json'

- script: |
    hexdump -C $(Pipeline.Workspace)/sonar_qube.json
  displayName: 'View content of sonar_qube.json using hexdump'



- pwsh: |
    # Read the json file into a PowerShell object
    $json = Get-Content -Path $(Pipeline.Workspace)/sonar_qube.json | ConvertFrom-Json
    # Extract the value of the keys 'sonar.host.url' and 'sonar.login'
    $url = $json.'sonar.host.url'
    $login = $json.'sonar.login'
    # Print the values to the console
    echo $url
    echo $login
    # If you want to use these values in other steps, you can set them as variables
    echo "##vso[task.setvariable variable=sonarUrl]$url"
    echo "##vso[task.setvariable variable=sonarLogin]$login"
  displayName: 'Extract values from JSON'



- task: SonarQubePrepare@4
  inputs:
    SonarQube: 'sonarqubedemo'
    scannerMode: 'CLI'
    configMode: 'manual'
    cliProjectKey: 'algimas'
    cliProjectName: 'algimas'
    cliSources: '$(Pipeline.Workspace)/s'
    extraProperties: |
      sonar.host.url=$(sonarUrl)
      sonar.login=$(sonarLogin)

- task: SonarQubeAnalyze@4

- task: SonarQubePublish@4
  inputs:
    pollingTimeoutSec: '300'
